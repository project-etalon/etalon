servers:
  - openai_server_engine: sglang
    openai_api_key: token-abc123
    profile_dir: prefill_experiments/prefill_profiler/llama-3-8b/sglang
    ttft_slack_slo: {
      "sharegpt": 0.9572672814596445,
      "arxiv": 0.89776420481503,
    }
  - openai_server_engine: default
    openai_api_key: token-abc123
    profile_dir: prefill_experiments/prefill_profiler/llama-3-8b/default
    ttft_slack_slo: {
      "sharegpt": 0.9572672814596445,
      "arxiv": 0.89776420481503,
    }

models:
  - name: llama-3-8b-instruct
    identifier: NousResearch/Meta-Llama-3-8B-Instruct
    parallel_specs: ["tp_1"]
    traces: null
  - name: llama-3-8b-32k
    identifier: NurtureAI/Meta-Llama-3-8B-Instruct-32k
    parallel_specs: ["tp_1"]
    traces: null

parallel_specs:
  - name: tp_1
    tp_dimension: 1
    pp_dimension: 1

request_generator_configs:
  - start_qps: 1
    request_interval_generator_provider: "poisson"
    request_length_generator_provider: "trace"
    trace_request_length_generator_trace_file: "data/processed_traces/sharegpt_8k_filtered_stats_llama2_tokenizer.csv"
    trace_file_name: "sharegpt"
  - start_qps: 1
    request_interval_generator_provider: "poisson"
    request_length_generator_provider: "trace"
    trace_request_length_generator_trace_file: "data/processed_traces/arxiv_summarization_filtered_stats_llama2_tokenizer.csv"
    trace_file_name: "arxiv"

request_configs:
  - num_ray_clients: 25
    num_concurrent_requests_per_client: 40
    timeout: 30
    max_num_completed_requests: 10
    additional_sampling_params: {}
    llm_api: "openai"
    request_generator_max_tokens: 8192
  - num_ray_clients: 25
    num_concurrent_requests_per_client: 40
    timeout: 30
    max_num_completed_requests: 10
    additional_sampling_params: {}
    llm_api: "openai"
    request_generator_max_tokens: 16384
